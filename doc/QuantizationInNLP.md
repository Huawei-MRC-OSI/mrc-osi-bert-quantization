### Transformer and related models

* A student report, covers BERT as well <http://web.stanford.edu/class/cs224n/reports/custom/15742249.pdf>
* [Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model](https://arxiv.org/abs/1906.00532)

### Word embeddings

* [Compressing Word Embeddings via Deep Compositional Code Learning](https://arxiv.org/abs/1711.01068)
  * The codebook trick can give an alternative approach to [memory layers](https://arxiv.org/abs/1907.05242)
* [Word2Bits - Quantized Word Vectors](https://arxiv.org/abs/1803.05651)
  * Impressive results, but may reflect Word2Vec being not so good more than quantization being good
  * Need to investigate applications to more up-to-date embeddings

### Surveys

* [A Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282)
* [Training Quantized Nets: A Deeper Understanding](https://arxiv.org/abs/1706.02379)
* [Efficient and Effective Quantization for Sparse DNNs](https://arxiv.org/abs/1903.03046)
* [Scalable Methods for 8-bit Training of Neural Networks](https://arxiv.org/abs/1805.11046)

### Additional papers

* [Convolutional neural network compression for natural language processing](https://arxiv.org/abs/1805.10796)
* [Improving text classification with vectors of reduced precision](https://arxiv.org/abs/1706.06363)
* [Neural Networks Compression for Language Modeling](https://arxiv.org/abs/1708.05963)
* [Natural Language Processing with Small Feed-Forward Networks](https://aclweb.org/anthology/D17-1309)
  * Not quantization, but building small networks in the first place, they can be quantized separately
